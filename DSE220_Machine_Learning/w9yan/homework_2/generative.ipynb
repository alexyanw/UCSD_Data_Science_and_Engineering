{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 2 - DSE 220: Machine Learning\n",
    "Download the MNIST train and test data from github along with their corresponding label files. The train and test data consist of 6000 and 1000 binarized MNIST images respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "X_train = pd.read_csv('mnist_train_data.csv', header=None)\n",
    "y_train = pd.read_csv('mnist_train_labels.csv', header=None)[0]\n",
    "X_test = pd.read_csv('mnist_test_data.csv', header=None)\n",
    "y_test = pd.read_csv('mnist_test_labels.csv', header=None)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative Learning\n",
    "Please don’t use the direct function from scikit-learn library for questions 1, 2, 3 and write your own implementation for them.\n",
    "\n",
    "**Question 1**: Compute and report the prior probabilities πj for all labels. (10 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prior probability for label 0 is 0.0987\n",
      "prior probability for label 1 is 0.1118\n",
      "prior probability for label 2 is 0.0968\n",
      "prior probability for label 3 is 0.1013\n",
      "prior probability for label 4 is 0.1038\n",
      "prior probability for label 5 is 0.0857\n",
      "prior probability for label 6 is 0.1013\n",
      "prior probability for label 7 is 0.1085\n",
      "prior probability for label 8 is 0.0918\n",
      "prior probability for label 9 is 0.1002\n"
     ]
    }
   ],
   "source": [
    "digit_counts = y_train.value_counts()\n",
    "total_count = len(y_train)\n",
    "labels = y_train.unique()\n",
    "labels.sort()\n",
    "priors = {}\n",
    "for j in labels:\n",
    "    priors[j] = digit_counts[j]/total_count\n",
    "    print(\"prior probability for label {} is {:.4f}\".format(j, priors[j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2**: For each pixel Xi and label j, compute Pji = P(Xi = 1|y = j) (Use\n",
    "the maximum likelihood estimate shown in class). Use Laplacian Smoothing for\n",
    "computing Pji. Report the highest Pji for each label j. (15 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "highest Pji for label 0 is 0.8519\n",
      "highest Pji for label 1 is 0.9851\n",
      "highest Pji for label 2 is 0.7290\n",
      "highest Pji for label 3 is 0.8082\n",
      "highest Pji for label 4 is 0.8496\n",
      "highest Pji for label 5 is 0.7112\n",
      "highest Pji for label 6 is 0.8492\n",
      "highest Pji for label 7 is 0.7948\n",
      "highest Pji for label 8 is 0.8752\n",
      "highest Pji for label 9 is 0.8673\n"
     ]
    }
   ],
   "source": [
    "num_pixels = X_train.shape[1]\n",
    "num_instance_j = np.zeros(len(labels))\n",
    "num_instance_ji = np.zeros((len(labels), num_pixels))\n",
    "P_ji = np.zeros((len(labels), num_pixels))\n",
    "for j in labels:\n",
    "    num_instance_j = digit_counts[j]\n",
    "    for i in range(num_pixels):\n",
    "        instances_j = X_train[y_train==j]\n",
    "        pixel_value_count = instances_j[i].value_counts()\n",
    "        num_instance_ji[j][i] = pixel_value_count[1.0] if 1.0 in pixel_value_count else 0\n",
    "        P_ji[j][i] = (num_instance_ji[j][i]+1)/(num_instance_j+2)\n",
    "    print(\"highest Pji for label {} is {:.4f}\".format(j, P_ji[j].max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3**: Use naive bayes (as shown in lecture slides) to classify the test\n",
    "data. Report the accuracy. (5 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy is: 0.8090\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def classify(x):\n",
    "    posterior = np.zeros(len(labels))\n",
    "    for j in labels:\n",
    "        posterior[j] = math.log(priors[j])\n",
    "        for i in range(num_pixels):\n",
    "            posterior[j] += x[i]*math.log(P_ji[j][i]) + (1 - x[i])*math.log(1-P_ji[j][i])\n",
    "    return np.argmax(posterior)\n",
    "y_pred = np.array([classify(X_test.T[i]) for i in range(len(X_test))])\n",
    "accuracy = np.sum(y_pred == y_test)*1.0/len(y_test)\n",
    "print(\"test accuracy is: {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: You can use the scikit-learn function from Question 4 onwards\n",
    "\n",
    "**Question 4**: Compute the confusion matrix (as shown in the lectures) and report\n",
    "the top 3 pairs with most (absolute number) incorrect classifications. (10 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 74   0   0   0   0   5   2   0   4   0]\n",
      " [  0 120   0   0   0   4   1   0   1   0]\n",
      " [  1   7  88   4   0   1   2   3   8   2]\n",
      " [  0   2   1  86   1   6   3   2   3   3]\n",
      " [  1   1   1   0  83   0   2   0   1  21]\n",
      " [  3   1   1  11   2  62   2   3   1   1]\n",
      " [  3   0   4   0   3   4  73   0   0   0]\n",
      " [  0   6   2   0   3   1   0  77   3   7]\n",
      " [  0   2   2   9   4   3   1   2  61   5]\n",
      " [  0   1   0   1   4   0   0   0   3  85]]\n",
      "top 3 pairs(true, predict) with most incorrect classification:\n",
      "[(4, 9), (5, 3), (8, 3)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "mat = confusion_matrix(y_test, y_pred)\n",
    "print(mat)\n",
    "mat1 = np.copy(mat)\n",
    "np.fill_diagonal(mat1,0)\n",
    "max_index = mat1.ravel().argsort()[-3:][::-1]\n",
    "top_pairs = np.unravel_index(max_index, mat.shape)\n",
    "print(\"top 3 pairs(true, predict) with most incorrect classification:\")\n",
    "print(list(zip(top_pairs[0], top_pairs[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5**: Visualizing mistakes: Print two MNIST images from the test data that your classifier misclassified. Write both the true and predicted labels for both of these misclassified digits. (10 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASwAAAClCAYAAADmtcDRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2105800c7b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "failed = X_test[y_test != y_pred]\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(5, 2*5))\n",
    "fig.subplots_adjust(hspace=0.6)\n",
    "for axi,i in enumerate(failed.index[:2]):\n",
    "    image = failed.T[i]\n",
    "    label,predict = y_test[i],y_pred[i]\n",
    "    title = \"label:{} predict:{}\".format(label,predict)\n",
    "    ax[axi].axes.get_xaxis().set_visible(False)\n",
    "    ax[axi].axes.get_yaxis().set_visible(False)\n",
    "    ax[axi].axes.get_xaxis().set_ticks([])\n",
    "    ax[axi].axes.get_yaxis().set_ticks([])\n",
    "    ax[axi].matshow(image.values.reshape(28,28))\n",
    "    ax[axi].set_title(title, size=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will implement Gaussian Mixture Model and Linear Discriminant Analysis on the breast cancer data (sklearn.datasets.load breast cancer) available in sklean.datasets. Load the data and split it into train-validation-test (40-20-40 split). Don’t shuffle the data, otherwise your results will be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "breast_cancer = datasets.load_breast_cancer()\n",
    "X = breast_cancer.data\n",
    "y = breast_cancer.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=5)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.33, random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6**: Implement Gaussian Mixture model on the data as shown in class. Tune the covariance type parameter on the validation data. Use the selected value to compute the test accuracy. As always, train the model on train+validation data to compute the test accuracy. (10 mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of components: 2\n",
      "val accuracy for full is 0.831858407079646\n",
      "val accuracy for tied is 0.8407079646017699\n",
      "val accuracy for diag is 0.9203539823008849\n",
      "val accuracy for spherical is 0.8849557522123894\n",
      "Best covariance type based on validation is: diag\n",
      "test accuracy is: 0.9167\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "labels = np.unique(y_train)\n",
    "print(\"number of components: {}\".format(len(labels)))\n",
    "\n",
    "accuracy = 0\n",
    "best_cov_type = ''\n",
    "for cov in ['full', 'tied', 'diag', 'spherical']:\n",
    "    clf = GaussianMixture(n_components=len(labels), covariance_type=cov)\n",
    "    clf.means_init = np.array([X_train[y_train == i].mean(axis=0)\n",
    "                                for i in labels])\n",
    "    clf.fit(X_train, y_train)\n",
    "    pred = clf.predict(X_val)\n",
    "    accu = accuracy_score(y_val, pred)\n",
    "    print(\"val accuracy for {} is {}\".format(cov, accu))\n",
    "    if accu > accuracy:\n",
    "        best_cov_type = cov\n",
    "        accuracy = accu\n",
    "print(\"Best covariance type based on validation is: {}\".format(best_cov_type))\n",
    "clf = GaussianMixture(n_components=len(labels), covariance_type=best_cov_type)\n",
    "X_train_valid = np.concatenate([X_train, X_val])\n",
    "y_train_valid = np.concatenate([y_train, y_val])\n",
    "clf.means_init = np.array([X_train_valid[y_train_valid == i].mean(axis=0) for i in labels])\n",
    "clf.fit(X_train_valid, y_train_valid)\n",
    "y_pred = clf.predict(X_test)\n",
    "test_accuracy = np.sum(y_pred == y_test)/len(y_test)\n",
    "print(\"test accuracy is: {:.4f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7**: Apply Linear Discriminant Analysis model on the train+validation data and report the accuracy obtained on test data. Report the transformation matrix (w) along with the intercept. (5 mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy = 0.9649\n",
      "The transformation matrix(w) is:\n",
      " [[  1.34830116e+00  -1.27341243e-01  -1.35844403e-01  -2.44423845e-04\n",
      "   -2.24362884e+01   8.56018583e+01   1.38546510e+01  -1.22119406e+02\n",
      "    1.60043667e+01   7.42980762e+00  -1.47932822e+01   4.82925722e-01\n",
      "    1.73467322e+00  -1.63578761e-02  -4.69433577e+02  -1.95981999e+00\n",
      "    1.60421869e+01  -9.77881013e+01  -6.13801471e+01   5.59104687e+02\n",
      "   -1.26194721e+00  -1.99981038e-01  -2.62323627e-01   1.92964678e-02\n",
      "    3.19490554e+01   4.09464513e+00  -1.26545911e+01  -9.32147959e+00\n",
      "   -1.48664190e+01  -1.71510006e+02]]\n",
      "The intercept is: [ 51.85532978]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# Intialize\n",
    "clf = LinearDiscriminantAnalysis()\n",
    "X_train_valid = np.concatenate((X_train, X_val))\n",
    "y_train_valid = np.concatenate((y_train, y_val))\n",
    "clf.fit(X_train_valid, y_train_valid)\n",
    "# Test\n",
    "y_pred = clf.predict(X_test)\n",
    "print('Test accuracy = {:.4f}'.format(np.sum(y_pred == y_test)/len(y_test)))\n",
    "print('The transformation matrix(w) is:\\n', clf.coef_)\n",
    "print('The intercept is:', clf.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Classifiers\n",
    "**Question 8**: Load the digits dataset (scikit-learn’s toy dataset) and take the last\n",
    "1300 samples as your test set. Train a K-Nearest Neighbor (k=5, linf distance)\n",
    "model and then without using any scikit-learn method, report the final values\n",
    "for Specificity, Sensitivity, TPR, TNR, FNR, FPR, Precision and Recall for\n",
    "Digit 3 (this digit is a positive, everything else is a negative). (15 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "X,y = digits.data, digits.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1300, random_state=5)\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=5, metric='chebyshev')\n",
    "clf.fit(X_train, y_train);\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporting metrics for digit 3:\n",
      "specificity: 0.995507637017071\n",
      "sensitivity: 0.916030534351145\n",
      "TPR: 0.916030534351145\n",
      "TNR: 0.995507637017071\n",
      "FNR: 0.08396946564885499\n",
      "FPR: 0.004492362982928988\n",
      "precision: 0.96\n",
      "recall: 0.916030534351145\n"
     ]
    }
   ],
   "source": [
    "metrics = {}\n",
    "TP = np.sum(np.logical_and(y_pred == y_test, y_pred == 3))\n",
    "TN = np.sum(np.logical_and(y_pred == y_test, y_test != 3))\n",
    "FN = np.sum(np.logical_and(y_pred != 3, y_test == 3))\n",
    "FP = np.sum(np.logical_and(y_pred == 3, y_test != 3))\n",
    "metrics['specificity'] = TN / (TN + FP)\n",
    "metrics['sensitivity'] = TP / (TP + FN)\n",
    "metrics['TPR'] = metrics['sensitivity']\n",
    "metrics['TNR'] = metrics['specificity']\n",
    "metrics['FNR'] = 1 - metrics['TPR']\n",
    "metrics['FPR'] = 1 - metrics['TNR']\n",
    "metrics['precision'] = TP / (TP + FP)\n",
    "metrics['recall'] = metrics['TPR']\n",
    "print(\"reporting metrics for digit 3:\")\n",
    "for k,v in metrics.items():\n",
    "    print(\"{}: {}\".format(k,v)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression\n",
    "An ablation experiment consists of removing one feature from an experiment,\n",
    "in order to assess the amount of additional information that feature provides\n",
    "above and beyond the others. For this section, we will use the diabetes dataset\n",
    "from scikit-learn’s toy datasets. Split the data into training and testing data as\n",
    "a 90-10 split with random state of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "diabetes = datasets.load_diabetes()\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 9**: Perform least squares regression on this dataset. Report the mean\n",
    "squared error and the mean absolute error on the test data. (5 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 2155.9646510319635\n",
      "MAE: 36.31813369867867\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# add one column\n",
    "X_train = np.c_[X_train, np.ones(len(X_train))]\n",
    "X_test = np.c_[X_test, np.ones(len(X_test))]\n",
    "\n",
    "# Least squares regression\n",
    "theta,residuals,rank,s = np.linalg.lstsq(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = np.dot(X_test, theta)\n",
    "\n",
    "# MSE calculation\n",
    "print (\"MSE: {}\".format(mean_squared_error(y_test, predictions)))\n",
    "# MAE calculation\n",
    "print (\"MAE: {}\".format(mean_absolute_error(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 10**: Repeat the experiment from Question 10 for all possible values of ablation (i.e., removing the feature 1 only, then removing the feature 2 only, and so on). Report all MSEs. (10 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE after removing feature 0 is : 2152.8066421806125\n",
      "MSE after removing feature 1 is : 2259.133079371277\n",
      "MSE after removing feature 2 is : 2783.514481845114\n",
      "MSE after removing feature 3 is : 2424.772348004414\n",
      "MSE after removing feature 4 is : 2187.599519380257\n",
      "MSE after removing feature 5 is : 2167.5176061492357\n",
      "MSE after removing feature 6 is : 2159.151482507473\n",
      "MSE after removing feature 7 is : 2153.0631711282294\n",
      "MSE after removing feature 8 is : 2335.1733846110847\n",
      "MSE after removing feature 9 is : 2165.8661921931885\n"
     ]
    }
   ],
   "source": [
    "mse = np.zeros(X_train.shape[1]-1)\n",
    "for i in range(X_train.shape[1]-1):\n",
    "    X_train1 = np.delete(X_train, i, 1)\n",
    "    X_test1 = np.delete(X_test, i, 1)\n",
    "    # Least squares regression\n",
    "    theta,residuals,rank,s = np.linalg.lstsq(X_train1, y_train)\n",
    "    # Make predictions on the test data\n",
    "    predictions = np.dot(X_test1, theta)\n",
    "    # MSE calculation\n",
    "    mse[i] = mean_squared_error(y_test, predictions)\n",
    "    print(\"MSE after removing feature {} is : {}\".format(i, mse[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 11**: Based on the MSE values obtained from Question 10, which features do you deem the most/least significant and why? (5 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most significant feature is index 2\n",
      "least significant feature is index 0\n"
     ]
    }
   ],
   "source": [
    "most_significant = np.argmax(mse)\n",
    "least_significant = np.argmin(mse)\n",
    "print(\"most significant feature is index {}\".format(most_significant))\n",
    "print(\"least significant feature is index {}\".format(least_significant))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
